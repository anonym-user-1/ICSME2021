{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''non_textual data classifiers library'''\n",
    "from __future__ import print_function\n",
    "import h2o\n",
    "from h2o.estimators import H2OXGBoostEstimator\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from h2o.estimators.stackedensemble import H2OStackedEnsembleEstimator\n",
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "\n",
    "'''textual data classifiers library'''\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, recall_score, precision_score, f1_score\n",
    "from scipy.sparse import hstack\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textual_load_data(project_name):\n",
    "    df = pd.read_parquet('Add your data path here')\n",
    "    train_df = df.loc[df['train_flag'] == 1]\n",
    "    test_df = df.loc[df['train_flag'] == 0]\n",
    "    \n",
    "#     load transformed tf_idf transformed Train data\n",
    "    with open('data/textual_data/project_base/transformed_train/commit/'+project_name+'.pickle', 'rb') as f:\n",
    "        train_transformed_commit = pickle.load(f) \n",
    "    with open('data/textual_data/project_base/transformed_train/issue/'+project_name+'.pickle', 'rb') as f:\n",
    "        train_transformed_issue = pickle.load(f) \n",
    "    with open('data/textual_data/project_base/transformed_train/code/'+project_name+'.pickle', 'rb') as f:\n",
    "        train_transformed_code = pickle.load(f)\n",
    "\n",
    "#     load transformed tf_idf transformed Test data\n",
    "    with open('data/textual_data/project_base/transformed_test/commit/'+project_name+'.pickle', 'rb') as f:\n",
    "        test_transformed_commit = pickle.load(f) \n",
    "    with open('data/textual_data/project_base/transformed_test/issue/'+project_name+'.pickle', 'rb') as f:\n",
    "        test_transformed_issue = pickle.load(f) \n",
    "    with open('data/textual_data/project_base/transformed_test/code/'+project_name+'.pickle', 'rb') as f:\n",
    "        test_transformed_code = pickle.load(f)\n",
    "    \n",
    "    train_transformed = hstack([train_transformed_issue,train_transformed_commit,train_transformed_code])\n",
    "    test_transformed = hstack([test_transformed_issue,test_transformed_commit,test_transformed_code])\n",
    "    \n",
    "    y_train = train_df['label'].copy().tolist()\n",
    "    y_test = test_df['label'].copy().tolist()\n",
    "    \n",
    "    return train_transformed, test_transformed, y_train, y_test\n",
    "\n",
    "def textual_run_gradient_boosting_model(project_name):\n",
    "    X_train, X_test, y_train, y_test = textual_load_data(project_name=project_name) \n",
    "#     Train the gradient boosting model\n",
    "    clf = GradientBoostingClassifier(n_estimators=300, max_features=None, learning_rate=0.1, max_depth=50, random_state=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    return clf.predict_proba(X_test), y_test   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_textual_ensemble_model(project_name):\n",
    "    train_path = 'Add your train data path here'\n",
    "    test_path = 'Add your test data path here'\n",
    "    predictors = [\"creator_key\", \"created_date\",\"updated_date\", \"last_resolved_date\", \"author\", \"committer\", \"author_time_date\", \n",
    "                          \"commit_time_date\",\"bug\", \"new feature\", \"task\", \"closed\", \"open\", \"resolved\"]\n",
    "    response_col = \"label\"\n",
    "    \n",
    "    train_df = h2o.import_file(train_path+project_name+'.parquet')\n",
    "    test_df = h2o.import_file(test_path+project_name+'.parquet')\n",
    "    \n",
    "    train_df['label'] = train_df['label'].asfactor()\n",
    "    train_df['creator_key'] = train_df['creator_key'].asfactor()\n",
    "    train_df['author'] = train_df['author'].asfactor()\n",
    "    train_df['committer'] = train_df['committer'].asfactor()\n",
    "    \n",
    "    test_df['label'] = test_df['label'].asfactor()\n",
    "    test_df['creator_key'] = test_df['creator_key'].asfactor()\n",
    "    test_df['author'] = test_df['author'].asfactor()\n",
    "    test_df['committer'] = test_df['committer'].asfactor()\n",
    "\n",
    "    train = train_df  \n",
    "    test = test_df \n",
    "    nfolds = 5\n",
    "    \n",
    "    # Train and cross-validate a GBM\n",
    "    my_gbm = H2OGradientBoostingEstimator(\n",
    "                                          distribution=\"bernoulli\",\n",
    "                                          ntrees=60,\n",
    "                                          max_depth=15,\n",
    "                                          min_rows=2,\n",
    "                                          learn_rate=0.1,\n",
    "                                          learn_rate_annealing=1,\n",
    "                                          nfolds=nfolds,\n",
    "                                          fold_assignment=\"Modulo\",\n",
    "                                          keep_cross_validation_predictions=True,\n",
    "                                          seed=1)\n",
    "    my_gbm.train(x=predictors, y=response_col, training_frame=train)\n",
    "\n",
    "    \n",
    "    # Train and cross-validate a xgboost\n",
    "    my_xgb = H2OXGBoostEstimator(booster='dart',\n",
    "                              distribution='bernoulli',\n",
    "                              ntrees=60,\n",
    "                              max_depth=20,\n",
    "                              min_rows=2,\n",
    "                              learn_rate=0.1,\n",
    "                              normalize_type=\"tree\",\n",
    "                              nfolds=nfolds,\n",
    "                              fold_assignment=\"Modulo\",\n",
    "                              keep_cross_validation_predictions=True,\n",
    "                              seed=1)\n",
    "    my_xgb.train(x=predictors, y=response_col, training_frame=train)\n",
    "\n",
    "\n",
    "    # Train a stacked ensemble using the GBM and XGB above\n",
    "    ensemble = H2OStackedEnsembleEstimator(model_id=\"my_ensemble_binomial\",\n",
    "                                           base_models=[my_gbm, my_xgb],\n",
    "                                           metalearner_nfolds=nfolds,\n",
    "                                           seed=1\n",
    "                                          )\n",
    "    ensemble.train(x=predictors, y=response_col, training_frame=train)\n",
    "\n",
    "    pred = ensemble.predict(test)\n",
    "    tmp1 = pred.as_data_frame()\n",
    "    tmp1.drop(columns=['predict'], inplace=True)\n",
    "    return tmp1.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(project_name):\n",
    "    h2o.init(max_mem_size=\"25G\")\n",
    "    \n",
    "    textual_x_pred, textual_y_test= textual_run_gradient_boosting_model(project_name=project_name)   \n",
    "    non_textual_x_pred = non_textual_ensemble_model(project_name=project_name)   \n",
    "    alpha = [0.05*i for i in range(0,21)]\n",
    "    results = pd.DataFrame(columns=['alpha','accuracy', 'recall', 'precision', 'f1'])\n",
    "    for i in range(len(alpha)):\n",
    "        output = []\n",
    "        x_pred = alpha[i]*non_textual_x_pred + (1-alpha[i])*textual_x_pred\n",
    "\n",
    "        pred = x_pred.argmax(axis=1)\n",
    "\n",
    "        df = {'alpha': alpha[i],\n",
    "              'accuracy': accuracy_score(textual_y_test, pred),\n",
    "              'recall': recall_score(textual_y_test, pred), \n",
    "              'precision': precision_score(textual_y_test, pred), \n",
    "              'f1':f1_score(textual_y_test, pred)}\n",
    "        results = results.append(df, ignore_index=True)\n",
    "        \n",
    "    print('best f1: ')\n",
    "    print(results[results.f1==np.max(results.f1.values)])\n",
    "    DF = pd.DataFrame(results[results.f1==np.max(results.f1.values)])\n",
    "    DF.to_csv('best f1 '+project_name+\".csv\")\n",
    "\n",
    "    h2o.remove_all()\n",
    "    h2o.shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble(project_name='Put your projects file name here')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
