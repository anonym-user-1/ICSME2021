{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "# -*- coding: UTF-8 -*-\n",
    "from gensim.models import word2vec, KeyedVectors, Word2Vec\n",
    "import preprocessor\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from decimal import *\n",
    "getcontext().prec = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='Your file name to save the logs', filemode='a', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "VECTOR_SIZE = 100\n",
    "TRAIN_ITERS = 300\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "HIDDEN_SIZE = 100\n",
    "N_INPUTS = 100\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "LSTM_KEEP_PROB = 0.9\n",
    "\n",
    "MAX_RECORD = {'step': -1, 'acc': 0.0}\n",
    "MAX_PRECISION = {'step': -1, 'acc': 0.0}\n",
    "MAX_RECALL = {'step': -1, 'acc': 0.0}\n",
    "MAX_F_MEASURE = {'step': -1, 'acc': 0.0}\n",
    "\n",
    "wordModel = KeyedVectors.load_word2vec_format('.bin file of word2vec of your data', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2vec(text, isHtml):\n",
    "    if isHtml:\n",
    "        seqs = preprocessor.processHTMLNoCamel(text)\n",
    "    else:\n",
    "        seqs = preprocessor.preprocessNoCamel(text)\n",
    "    res = []\n",
    "    for seq in seqs:\n",
    "        for word in seq:\n",
    "            try:\n",
    "                res.append(wordModel[word])\n",
    "            except KeyError:\n",
    "                res.append(np.zeros(VECTOR_SIZE))\n",
    "    return res\n",
    "\n",
    "# shape = [None, seq len, Vec size]\n",
    "def read_data(df):\n",
    "    X1 = []\n",
    "    X2 = []\n",
    "    T = []\n",
    "    L1 = []\n",
    "    L2 = []\n",
    "    LT = []\n",
    "    Y = []\n",
    "    logging.info(\"Loaded the file\")\n",
    "    for index, row in df.iterrows():\n",
    "        commit = text2vec(row['commit'], False)\n",
    "        issue = text2vec(row['issue'], True)\n",
    "        title = text2vec(row['issue_title'], False)\n",
    "        if len(commit) < 5:\n",
    "            continue\n",
    "        if len(issue)+len(title) < 5:\n",
    "            continue\n",
    "        L1.append(len(commit))\n",
    "        X1.append(commit)\n",
    "        L2.append(len(issue))\n",
    "        X2.append(issue)\n",
    "        LT.append(len(title))\n",
    "        T.append(title)\n",
    "        Y.append(float(row['labels']))\n",
    "        gc.collect()\n",
    "    logging.info(\"reading data is done\")\n",
    "    return X1, X2, T, L1, L2, LT, Y\n",
    "\n",
    "\n",
    "# shape=[batch_size, None]\n",
    "def make_batches(data, batch_size):\n",
    "    logging.info(\"batching is starting\")\n",
    "    X1, X2, T, L1, L2, LT, Y = data\n",
    "    num_batches = len(Y) // batch_size\n",
    "    data1 = np.array(X1[: batch_size*num_batches])\n",
    "    data1 = np.reshape(data1, [batch_size, num_batches])\n",
    "    data_batches1 = np.split(data1, num_batches, axis=1)  #  list\n",
    "    data_batches1_rs = []\n",
    "    for d1 in data_batches1:\n",
    "        sub_batch = []\n",
    "        maxD = 0\n",
    "        for d in d1:\n",
    "            for dt in d:\n",
    "                maxD = max(maxD, len(dt))\n",
    "        for d in d1:\n",
    "            for dt in d:\n",
    "                todo = maxD - len(dt)\n",
    "                for index in range(todo):\n",
    "                    dt.append(np.zeros(VECTOR_SIZE))\n",
    "                sub_batch.append(np.array(dt))\n",
    "        data_batches1_rs.append(np.array(sub_batch))\n",
    "\n",
    "    data2 = np.array(X2[: batch_size*num_batches])\n",
    "    data2 = np.reshape(data2, [batch_size, num_batches])\n",
    "    data_batches2 = np.split(data2, num_batches, axis=1)\n",
    "    data_batches2_rs = []\n",
    "    for d2 in data_batches2:\n",
    "        sub_batch = []\n",
    "        maxD = 0\n",
    "        for d in d2:\n",
    "            for dt in d:\n",
    "                maxD = max(maxD, len(dt))\n",
    "        for d in d2:\n",
    "            for dt in d:\n",
    "                todo = maxD - len(dt)\n",
    "                for index in range(todo):\n",
    "                    dt.append(np.zeros(VECTOR_SIZE))\n",
    "                sub_batch.append(np.array(dt))\n",
    "        data_batches2_rs.append(np.array(sub_batch))\n",
    "\n",
    "    dataT = np.array(T[: batch_size*num_batches])\n",
    "    dataT = np.reshape(dataT, [batch_size, num_batches])\n",
    "    data_batchesT = np.split(dataT, num_batches, axis=1)  #  list\n",
    "    data_batchesT_rs = []\n",
    "    for d3t in data_batchesT:\n",
    "        sub_batch = []\n",
    "        maxD = 0\n",
    "        for d in d3t:\n",
    "            for dt in d:\n",
    "                maxD = max(maxD, len(dt))\n",
    "        for d in d3t:\n",
    "            for dt in d:\n",
    "                todo = maxD - len(dt)\n",
    "                for index in range(todo):\n",
    "                    dt.append(np.zeros(VECTOR_SIZE))\n",
    "                sub_batch.append(np.array(dt))\n",
    "        data_batchesT_rs.append(np.array(sub_batch))\n",
    "\n",
    "    len1 = np.array(L1[: batch_size*num_batches])\n",
    "    len1 = np.reshape(len1, [batch_size, num_batches])\n",
    "    len_batches1 = np.split(len1, num_batches, axis=1)\n",
    "    len_batches1 = np.reshape(np.array(len_batches1), [num_batches, BATCH_SIZE])\n",
    "\n",
    "    len2 = np.array(L2[: batch_size * num_batches])\n",
    "    len2 = np.reshape(len2, [batch_size, num_batches])\n",
    "    len_batches2 = np.split(len2, num_batches, axis=1)\n",
    "    len_batches2 = np.reshape(np.array(len_batches2), [num_batches, BATCH_SIZE])\n",
    "\n",
    "    lenT = np.array(LT[: batch_size * num_batches])\n",
    "    lenT = np.reshape(lenT, [batch_size, num_batches])\n",
    "    len_batchesT = np.split(lenT, num_batches, axis=1)\n",
    "    len_batchesT = np.reshape(np.array(len_batchesT), [num_batches, BATCH_SIZE])\n",
    "\n",
    "    label = np.array(Y[: batch_size*num_batches])\n",
    "    label = np.reshape(label, [batch_size, num_batches])\n",
    "    label_batches = np.split(label, num_batches, axis=1)\n",
    "    logging.info(\"batching is done!!!\")\n",
    "    return list(zip(data_batches1_rs, data_batches2_rs, data_batchesT_rs, len_batches1, len_batches2, len_batchesT, label_batches))\n",
    "\n",
    "\n",
    "class MyModel(object):\n",
    "    def __init__(self, is_training, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.input1 = tf.compat.v1.placeholder(tf.float32, [BATCH_SIZE, None, VECTOR_SIZE])\n",
    "        self.input2 = tf.compat.v1.placeholder(tf.float32, [BATCH_SIZE, None, VECTOR_SIZE])\n",
    "        self.inputT = tf.compat.v1.placeholder(tf.float32, [BATCH_SIZE, None, VECTOR_SIZE])\n",
    "        self.len1 = tf.compat.v1.placeholder(tf.int32, [BATCH_SIZE, ])\n",
    "        self.len2 = tf.compat.v1.placeholder(tf.int32, [BATCH_SIZE, ])\n",
    "        self.lent = tf.compat.v1.placeholder(tf.int32, [BATCH_SIZE, ])\n",
    "        self.target = tf.compat.v1.placeholder(tf.float32, [BATCH_SIZE, 1])\n",
    "\n",
    "        with tf.compat.v1.variable_scope(\"commit\"):\n",
    "            outputs1, states1 = self.RNN(self.input1, self.len1, is_training)\n",
    "        with tf.compat.v1.variable_scope(\"issue\"):\n",
    "            outputs2, states2 = self.RNN(self.input2, self.len2, is_training)\n",
    "        with tf.compat.v1.variable_scope(\"title\"):\n",
    "            outputs3, states3 = self.RNN(self.inputT, self.lent, is_training)\n",
    "\n",
    "        newoutput1 = states1[-1].h\n",
    "        newoutput2 = states2[-1].h\n",
    "        newoutput3 = states3[-1].h\n",
    "\n",
    "        # Define loss and optimizer\n",
    "        self.cos_score = self.getScore(newoutput1, newoutput2, newoutput3)\n",
    "        self.loss_op = self.getLoss(self.cos_score, self.target)\n",
    "\n",
    "        if not is_training:\n",
    "            return\n",
    "\n",
    "        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "        self.train_op = optimizer.minimize(self.loss_op)\n",
    "\n",
    "    def getScore(self, state1, state2, state3):\n",
    "        pooled_len_1 = tf.sqrt(tf.reduce_sum(state1 * state1, 1))\n",
    "        pooled_len_2 = tf.sqrt(tf.reduce_sum(state2 * state2, 1))\n",
    "        pooled_mul_12 = tf.reduce_sum(state1 * state2, 1)\n",
    "        score1 = tf.compat.v1.div(pooled_mul_12, pooled_len_1 * pooled_len_2 + 1e-8, name=\"scores1\")  # +1e-8 avoid 'len_1/len_2 == 0'\n",
    "        score1 = tf.compat.v1.reshape(score1, [BATCH_SIZE, 1])\n",
    "\n",
    "        pooled_len_3 = tf.sqrt(tf.reduce_sum(state3 * state3, 1))\n",
    "        pooled_mul_13 = tf.reduce_sum(state1 * state3, 1)\n",
    "        score2 = tf.compat.v1.div(pooled_mul_13, pooled_len_1 * pooled_len_3 + 1e-8, name=\"scores2\")  # +1e-8 avoid 'len_1/len_2 == 0'\n",
    "        score2 = tf.compat.v1.reshape(score2, [BATCH_SIZE, 1])\n",
    "\n",
    "        score = tf.compat.v1.concat([score1, score2], 1)\n",
    "        score = tf.compat.v1.reduce_max(score, 1)\n",
    "        return tf.compat.v1.reshape(score, [BATCH_SIZE, 1])\n",
    "\n",
    "    #  |t - cossimilar(state1, state2)|\n",
    "    def getLoss(self, score, t):\n",
    "        rs = t - score\n",
    "        rs = tf.compat.v1.abs(rs)\n",
    "        return tf.compat.v1.reduce_sum(rs)\n",
    "\n",
    "    def RNN(self, input_data, seq_len, is_training):\n",
    "        dropout_keep_prob = LSTM_KEEP_PROB if is_training else 1.0\n",
    "        lstm_cells = [\n",
    "            tf.compat.v1.nn.rnn_cell.DropoutWrapper(tf.compat.v1.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE), output_keep_prob=dropout_keep_prob)\n",
    "            for _ in range(1)\n",
    "        ]\n",
    "        rnn_cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell(lstm_cells)\n",
    "        outputs, state = tf.compat.v1.nn.dynamic_rnn(rnn_cell, input_data, sequence_length=seq_len, dtype=tf.float32)\n",
    "        return outputs, state\n",
    "\n",
    "\n",
    "def run_epoch(session, model, batches, step):\n",
    "    # session.run(model.init_state)\n",
    "    for x1, x2, t, l1, l2, lt, y in batches:\n",
    "        loss, _ = session.run([model.loss_op, model.train_op],\n",
    "                           feed_dict={model.input1: x1, model.input2: x2, model.inputT: t, model.len1: l1, model.len2: l2, model.lent: lt, model.target: y})\n",
    "        logging.info(\"At the step %d, the loss is %f\" % (step, loss))\n",
    "\n",
    "\n",
    "def test_epoch(session, model, batches, step):\n",
    "    temp = []\n",
    "    total_correct = 0\n",
    "    total_tests = len(batches) * BATCH_SIZE\n",
    "    index = 0\n",
    "    total_TP = 0\n",
    "    total_TN = 0\n",
    "    total_FP = 0\n",
    "    total_FN = 0\n",
    "    for x11, x21, t1, l11, l21, lt1, y1 in batches:\n",
    "        score, loss = session.run([model.cos_score, model.loss_op],\n",
    "                                feed_dict={model.input1: x11, model.input2: x21, model.inputT: t1, model.len1: l11, model.len2: l21, model.lent: lt1,\n",
    "                                           model.target: y1})\n",
    "        temp.append(loss)\n",
    "        total_correct = total_correct + get_correct(score, y1, index, len(batches))\n",
    "        index = index + 1\n",
    "        measure = get_measure(score, y1)\n",
    "        total_TP = total_TP + measure[0]\n",
    "        total_TN = total_TN + measure[1]\n",
    "        total_FP = total_FP + measure[2]\n",
    "        total_FN = total_FN + measure[3]\n",
    "\n",
    "    precision = float(total_TP) / (total_TP + total_FP+1e-8)\n",
    "    recall = float(total_TP) / (total_TP + total_FN+1e-8)\n",
    "    if precision==0 and recall==0:\n",
    "        f_measure=0\n",
    "    else:\n",
    "        f_measure = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    logging.info(\"At the test %d, the avg loss is %f, the accuracy is %f\" % (step, np.mean(np.array(temp)), float(total_correct) / total_tests))\n",
    "    logging.info(\"At the test %d, TP:%d TN:%d FP:%d FN:%d\" % (step, total_TP, total_TN, total_FP, total_FN))\n",
    "    logging.info(\"At the test %d, precision:%f recall:%f f_measure:%f\" % (step, precision, recall, f_measure))\n",
    "    if (float(total_correct) / total_tests) > MAX_RECORD['acc']:\n",
    "        MAX_RECORD['step'] = step\n",
    "        MAX_RECORD['acc'] = float(total_correct) / total_tests\n",
    "    if precision > MAX_PRECISION['acc']:\n",
    "        MAX_PRECISION['step'] = step\n",
    "        MAX_PRECISION['acc'] = precision\n",
    "    if recall > MAX_RECALL['acc']:\n",
    "        MAX_RECALL['step'] = step\n",
    "        MAX_RECALL['acc'] = recall\n",
    "    if f_measure > MAX_F_MEASURE['acc']:\n",
    "        MAX_F_MEASURE['step'] = step\n",
    "        MAX_F_MEASURE['acc'] = f_measure\n",
    "    logging.info(\"MAX is at step %d: %f\" % (MAX_RECORD['step'], MAX_RECORD['acc']))\n",
    "    logging.info(\"MAX precision is at step %d: %f\" % (MAX_PRECISION['step'], MAX_PRECISION['acc']))\n",
    "    logging.info(\"MAX recall is at step %d: %f\" % (MAX_RECALL['step'], MAX_RECALL['acc']))\n",
    "    logging.info(\"MAX f_measure is at step %d: %f\" % (MAX_F_MEASURE['step'], MAX_F_MEASURE['acc']))\n",
    "\n",
    "\n",
    "def get_correct(score, target, index, NUM):\n",
    "    result = 0\n",
    "    for i in range(len(target)):\n",
    "        if target[i][0] == 1 and score[i][0] > 0.5:\n",
    "            result = result + 1\n",
    "        elif target[i][0] == 0 and score[i][0] < 0.5:\n",
    "            result = result + 1\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_measure(score, target):\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for i in range(len(target)):\n",
    "        if target[i][0] == 1:\n",
    "            if score[i][0] > 0.5:\n",
    "                TP = TP+1\n",
    "            else:\n",
    "                FN = FN+1\n",
    "        elif target[i][0] == 0:\n",
    "            if score[i][0] < 0.5:\n",
    "                TN = TN+1\n",
    "            else:\n",
    "                FP = FP+1\n",
    "    return TP, TN, FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    data = pd.read_parquet('Your Project file')\n",
    "    df_train, df_test = train_test_split(data, test_size=0.2)\n",
    "    \n",
    "#     batching\n",
    "    train_batches = make_batches(read_data(df=df_train), BATCH_SIZE)\n",
    "    test_batches = make_batches(read_data(df=df_test), BATCH_SIZE)\n",
    "#     creating model\n",
    "    with tf.compat.v1.variable_scope(\"rnn_model\", reuse=None):\n",
    "        train_model = MyModel(True, BATCH_SIZE)\n",
    "    with tf.compat.v1.variable_scope(\"rnn_model\", reuse=True):\n",
    "        test_model = MyModel(False, BATCH_SIZE)\n",
    "#     Training the model\n",
    "    init = tf.compat.v1.global_variables_initializer()\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        saver = tf.compat.v1.train.Saver()\n",
    "        sess.run(init)\n",
    "        logging.info(\"Test Set: %d\" % (len(test_batches) * BATCH_SIZE))\n",
    "\n",
    "        for step in range(TRAIN_ITERS):\n",
    "            logging.info(\"Step: \" + str(step))\n",
    "            run_epoch(session=sess, model=train_model, batches=train_batches, step=step)\n",
    "            test_epoch(session=sess, model=test_model, batches=test_batches, step=step)\n",
    "        saver.save(sess, 'Path to save the model chack point', global_step=TRAIN_ITERS)\n",
    "        logging.info(\"Optimization Finished!\")\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
